{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9829c864",
   "metadata": {},
   "source": [
    "# PyTorch Workflow\n",
    "\n",
    "Let's explore PyTorch end-to-end workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b7771f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu117\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a7c7ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'data(prepare and load)',\n",
       " 2: 'build model',\n",
       " 3: 'fitting the model (training)',\n",
       " 4: 'making predictions and evaluting model (inference)',\n",
       " 5: 'putting it all together'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Workflow \n",
    "what_were_covering = {1: \"data(prepare and load)\",\n",
    "                      2: \"build model\",\n",
    "                      3: \"fitting the model (training)\",\n",
    "                      4: \"making predictions and evaluting model (inference)\",\n",
    "                      5: \"putting it all together\"}\n",
    "what_were_covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1d4ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch and cuda version: 1.13.1+cu117\n",
      "Accelerator/Cuda/GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn # nn contains all PyTorch's building blocks for neural networks \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Check python version\n",
    "print(f\"PyTorch and cuda version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Accelerator/Cuda/GPU is available!\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Accelerator/Cuda/GPU is not available ;C, CPU mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1428bd",
   "metadata": {},
   "source": [
    "## 1. Data (preparing and loading)\n",
    "\n",
    "* Excel \n",
    "* Images\n",
    "* Videos\n",
    "* Audio \n",
    "* DNA \n",
    "* Text\n",
    "* (...)\n",
    "\n",
    "Machine learning: \n",
    "1. Get data into numerical representation\n",
    "2. Build model to learn patterns in that numerical representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bcc09c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create artificial data with know pattern (linnear function)\n",
    "weight = 0.7 \n",
    "bias = 0.3\n",
    "\n",
    "# Create\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # Adds extra dimension, sol we have (1,10)\n",
    "y = weight * X + bias\n",
    "\n",
    "X[:10], y[:10] # That notation is to show the first 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "76089e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y) # 50 elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5f42b45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 40, 10)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spliting data into train, test, validation (basic machine learning step)\n",
    "# Guide:\n",
    "# Train ->  60-80%\n",
    "# Validation -> 10-20%\n",
    "# Test -> 10-20%\n",
    "\n",
    "# Note: Train + validation = design split/partition\n",
    "\n",
    "train_split = int(0.8 * len(X)) # 50 * 0.8\n",
    "X_train, y_train  = X[:train_split], y[:train_split] # Taking first 50*0.8 data\n",
    "X_test, y_test = X[train_split:], y[train_split:] # Taking the rest \n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)\n",
    "# 40 data to train, 10 to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "792c2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "    '''\n",
    "    Plots training data, test data and compares predictions\n",
    "    '''\n",
    "\n",
    "    # Plot training data in blue \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=8, label=\"Training data\")\n",
    "    \n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=8 ,label=\"Test data\")\n",
    "    \n",
    "    # Â¿Predictions? -> Plot if exists\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=8, label=\"Predictions\")\n",
    "    \n",
    "    \n",
    "    # Show the legend\n",
    "    plt.legend(prop={\"size\":14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0eb8ecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6d40f",
   "metadata": {},
   "source": [
    "## 2. Bulding first PyTorch model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "31ca1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regresion model class\n",
    "\n",
    "class LinearRegressionModel(torch.nn.Module): ## Almost everything in PyTorch inherist from nn.Module\n",
    "    # Class init \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        ############### Initialize parameters ###############\n",
    "        \n",
    "            # We use the sublcass nn.Parameter to create the, obmf course, parameters\n",
    "        self.weights = nn.Parameter(torch.randn(1,\n",
    "                                                requires_grad=True,\n",
    "                                                dtype=torch.float)) # Creates attribute \"weights\" \n",
    "            # Parameter for bias\n",
    "        self.bias = nn.Parameter(torch.randn(1,\n",
    "                                             requires_grad=True,\n",
    "                                             dtype=torch.float))    \n",
    "         \n",
    "    # Forward method to define the computation in the model at every call\n",
    "    # Important: it's not inside \"__init__\" function\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor: # x is the input data\n",
    "        return  self.weights * x + self.bias # Linear regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e34b5",
   "metadata": {},
   "source": [
    "### PyTorch model building essentials\n",
    "* torch.nn -> contains all of the buildings for computational graph (the neural network)\n",
    "* torch.nn.Parameter -> what parameters should our model try and learn, often a PyTorch layer will set this\n",
    "* torch.nn.Module -> The base class for all neural networks, if you subclass it, you should overwrite forward(). \n",
    "* torch.optim -> PyTorch optimizers (will help Gradient Descent)\n",
    "* def forward() -> All nn.Module subclasses requires you to overwrite it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e276c5a",
   "metadata": {},
   "source": [
    "### Checking the content of our PyTorch model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7b26210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random seed \n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the model (subclass of nn.Module)\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check parameters \n",
    "list(model_0.parameters()) # Without list .parameters() give a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b32607f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List named parameters \n",
    "model_0.state_dict() # Diccionary of the model parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485769f",
   "metadata": {},
   "source": [
    "### Making predictions using `torch.inference_mode()`\n",
    "\n",
    "When we pass data, it's going to run it through the `forward()` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5ba31afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple way: y_preds = model_0(X_test)\n",
    "with torch.inference_mode(): # -> Contex manager \n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "y_preds\n",
    "# Note: inference mode don`t track gradient (interesting for large dataset, faster computation/less computation)\n",
    "# Note 2: inference mode it`s the prefered way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "adf5fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way (but with less advantages)\n",
    "with torch.no_grad():\n",
    "    y_preds2 = model_0(X_test)\n",
    "y_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1037a9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8600],\n",
       "        [0.8740],\n",
       "        [0.8880],\n",
       "        [0.9020],\n",
       "        [0.9160],\n",
       "        [0.9300],\n",
       "        [0.9440],\n",
       "        [0.9580],\n",
       "        [0.9720],\n",
       "        [0.9860]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52bc3d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af41d7",
   "metadata": {},
   "source": [
    "## 3. Train model \n",
    "\n",
    "* Idea: start random parameters -> Poor representation -> Upgrade representation \n",
    "* How to measure how good is your model: cost/loss function (also called criteria) -> The lower the better\n",
    "\n",
    "What we need in PyTorch: \n",
    "1. Training loop\n",
    "2. Testing loop  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cedc8368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fbd09373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1Loss()\n"
     ]
    }
   ],
   "source": [
    "# Setup a loss function (MAE)\n",
    "loss_fn = nn.L1Loss()\n",
    "print(loss_fn)\n",
    "\n",
    "# Setup a optimizer (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), # Our model parameters \n",
    "                           lr=0.01)                      # Learning rate, very important "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb17eb0",
   "metadata": {},
   "source": [
    "** Which loss and optimizer should I use? **\n",
    "Usually:\n",
    "* For regresion: `nn.L1Loss()` or `nn.MSE()` for loss and `torch.optim.SGD()` for optimizer will suffice.\n",
    "* For classification: `nn.BCE()`(binary cross entropy) will work.\n",
    "\n",
    "* **Note**: this is just a \"initial setup\", you will need to experiment with different ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9c58a",
   "metadata": {},
   "source": [
    "### Building the training loop\n",
    "\n",
    "0. Loop through the data\n",
    "1. Forward pass (data moving through our model's  `forward()`) -> **Forward propagation**\n",
    "2. Calculate loss (compare predictions to real values/labels)\n",
    "3. Optimizer zero grad \n",
    "4. Loss backward (Move backwards through the network to calculate the gradients of our model with respect to the loss)\n",
    "5. Optimizer step (use optimizer to adjust model parameters to improve loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fcd2eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8bc184ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Epoch 0 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 1 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 2 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 3 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 4 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 5 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 6 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 7 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 8 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 9 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 10 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 11 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 12 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 13 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 14 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 15 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 16 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 17 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 18 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 19 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 20 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 21 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 22 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 23 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 24 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 25 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 26 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 27 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 28 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 29 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 30 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 31 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 32 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 33 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 34 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 35 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 36 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 37 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 38 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 39 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 40 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 41 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 42 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 43 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 44 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 45 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 46 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 47 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 48 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 49 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 50 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 51 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 52 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 53 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 54 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 55 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 56 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 57 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 58 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 59 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 60 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 61 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 62 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 63 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 64 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 65 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 66 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 67 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 68 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 69 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 70 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 71 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 72 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 73 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 74 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 75 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 76 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 77 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 78 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 79 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 80 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 81 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 82 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 83 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 84 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 85 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 86 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 87 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 88 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 89 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 90 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 91 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 92 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 93 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 94 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 95 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 96 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 97 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 98 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 99 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 100 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 101 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 102 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 103 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 104 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 105 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 106 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 107 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 108 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 109 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 110 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 111 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 112 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 113 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 114 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 115 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 116 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 117 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 118 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 119 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 120 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 121 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 122 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 123 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 124 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 125 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 126 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 127 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 128 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 129 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 130 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 131 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 132 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 133 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 134 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 135 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 136 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 137 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 138 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 139 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 140 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 141 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 142 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 143 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 144 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 145 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 146 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 147 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 148 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 149 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 150 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 151 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 152 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 153 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 154 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 155 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 156 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 157 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 158 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 159 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 160 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 161 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 162 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 163 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 164 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 165 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 166 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 167 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 168 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 169 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 170 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 171 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 172 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 173 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 174 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 175 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 176 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 177 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 178 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 179 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 180 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 181 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 182 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 183 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 184 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 185 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 186 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 187 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 188 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 189 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 190 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 191 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 192 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 193 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 194 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 195 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 196 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 197 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 198 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 199 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 200 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 201 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 202 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 203 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 204 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 205 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 206 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 207 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 208 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 209 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 210 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 211 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 212 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 213 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 214 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 215 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 216 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 217 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 218 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 219 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 220 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 221 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 222 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 223 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 224 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 225 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 226 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 227 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 228 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 229 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 230 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 231 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 232 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 233 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 234 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 235 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 236 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 237 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 238 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 239 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 240 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 241 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 242 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 243 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 244 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 245 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 246 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 247 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 248 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 249 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 250 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 251 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 252 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 253 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 254 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 255 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 256 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 257 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 258 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 259 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 260 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 261 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 262 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 263 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 264 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 265 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 266 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 267 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 268 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 269 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 270 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 271 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 272 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 273 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 274 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 275 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 276 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 277 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 278 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 279 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 280 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 281 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 282 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 283 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 284 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 285 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 286 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 287 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 288 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 289 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 290 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 291 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 292 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 293 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 294 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 295 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 296 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 297 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 298 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 299 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 300 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 301 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 302 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 303 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 304 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 305 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 306 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 307 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 308 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 309 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 310 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 311 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 312 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 313 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 314 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 315 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 316 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 317 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 318 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 319 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 320 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 321 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 322 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 323 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 324 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 325 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 326 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 327 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 328 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 329 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 330 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 331 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 332 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 333 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 334 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 335 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 336 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 337 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 338 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 339 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 340 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 341 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 342 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 343 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 344 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 345 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 346 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 347 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 348 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 349 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 350 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 351 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 352 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 353 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 354 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 355 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 356 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 357 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 358 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 359 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 360 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 361 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 362 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 363 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 364 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 365 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 366 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 367 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 368 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 369 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 370 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 371 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 372 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 373 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 374 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 375 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 376 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 377 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 378 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 379 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 380 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 381 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 382 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 383 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 384 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 385 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 386 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 387 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 388 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 389 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 390 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 391 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 392 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 393 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 394 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 395 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 396 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 397 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 398 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 399 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 400 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 401 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 402 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 403 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 404 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 405 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 406 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 407 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 408 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 409 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 410 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 411 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 412 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 413 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 414 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 415 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 416 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 417 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 418 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 419 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 420 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 421 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 422 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 423 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 424 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 425 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 426 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 427 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 428 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 429 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 430 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 431 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 432 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 433 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 434 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 435 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 436 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 437 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 438 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 439 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 440 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 441 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 442 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 443 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 444 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 445 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 446 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 447 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 448 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 449 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 450 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 451 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 452 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 453 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 454 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 455 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 456 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 457 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 458 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 459 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 460 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 461 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 462 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 463 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 464 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 465 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 466 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 467 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 468 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 469 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 470 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 471 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 472 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 473 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 474 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 475 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 476 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 477 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 478 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 479 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 480 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 481 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 482 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 483 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 484 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 485 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 486 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 487 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 488 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 489 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 490 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 491 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 492 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 493 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 494 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 495 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 496 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 497 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 498 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 499 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 500 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 501 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 502 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 503 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 504 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 505 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 506 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 507 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 508 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 509 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 510 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 511 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 512 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 513 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 514 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 515 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 516 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 517 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 518 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 519 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 520 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 521 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 522 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 523 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 524 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 525 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 526 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 527 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 528 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 529 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 530 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 531 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 532 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 533 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 534 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 535 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 536 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 537 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 538 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 539 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 540 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 541 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 542 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 543 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 544 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 545 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 546 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 547 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 548 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 549 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 550 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 551 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 552 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 553 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 554 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 555 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 556 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 557 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 558 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 559 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 560 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 561 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 562 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 563 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 564 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 565 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 566 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 567 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 568 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 569 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 570 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 571 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 572 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 573 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 574 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 575 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 576 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 577 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 578 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 579 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 580 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 581 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 582 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 583 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 584 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 585 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 586 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 587 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 588 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 589 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 590 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 591 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 592 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 593 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 594 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 595 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 596 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 597 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 598 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 599 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 600 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 601 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 602 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 603 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 604 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 605 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 606 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 607 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 608 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 609 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 610 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 611 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 612 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 613 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 614 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 615 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 616 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 617 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 618 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 619 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 620 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 621 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 622 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 623 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 624 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 625 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 626 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 627 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 628 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 629 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 630 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 631 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 632 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 633 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 634 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 635 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 636 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 637 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 638 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 639 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 640 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 641 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 642 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 643 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 644 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 645 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 646 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 647 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 648 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 649 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 650 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 651 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 652 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 653 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 654 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 655 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 656 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 657 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 658 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 659 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 660 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 661 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 662 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 663 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 664 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 665 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 666 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 667 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 668 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 669 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 670 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 671 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 672 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 673 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 674 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 675 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 676 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 677 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 678 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 679 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 680 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 681 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 682 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 683 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 684 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 685 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 686 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 687 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 688 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 689 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 690 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 691 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 692 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 693 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 694 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 695 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 696 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 697 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 698 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 699 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 700 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 701 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 702 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 703 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 704 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 705 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 706 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 707 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 708 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 709 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 710 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 711 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 712 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 713 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 714 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 715 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 716 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 717 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 718 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 719 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 720 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 721 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 722 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 723 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 724 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 725 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 726 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 727 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 728 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 729 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 730 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 731 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 732 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 733 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 734 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 735 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 736 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 737 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 738 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 739 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 740 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 741 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 742 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 743 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 744 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 745 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 746 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 747 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 748 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 749 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 750 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 751 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 752 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 753 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 754 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 755 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 756 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 757 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 758 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 759 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 760 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 761 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 762 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 763 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 764 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 765 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 766 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 767 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 768 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 769 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 770 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 771 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 772 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 773 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 774 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 775 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 776 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 777 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 778 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 779 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 780 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 781 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 782 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 783 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 784 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 785 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 786 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 787 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 788 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 789 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 790 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 791 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 792 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 793 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 794 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 795 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 796 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 797 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 798 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 799 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 800 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 801 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 802 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 803 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 804 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 805 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 806 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 807 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 808 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 809 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 810 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 811 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 812 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 813 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 814 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 815 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 816 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 817 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 818 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 819 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 820 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 821 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 822 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 823 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 824 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 825 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 826 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 827 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 828 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 829 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 830 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 831 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 832 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 833 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 834 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 835 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 836 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 837 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 838 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 839 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 840 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 841 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 842 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 843 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 844 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 845 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 846 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 847 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 848 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 849 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 850 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 851 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 852 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 853 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 854 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 855 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 856 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 857 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 858 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 859 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 860 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 861 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 862 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 863 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 864 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 865 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 866 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 867 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 868 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 869 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 870 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 871 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 872 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 873 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 874 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 875 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 876 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 877 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 878 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 879 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 880 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 881 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 882 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 883 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 884 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 885 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 886 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 887 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 888 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 889 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 890 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 891 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 892 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 893 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 894 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 895 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 896 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 897 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 898 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 899 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 900 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 901 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 902 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 903 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 904 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 905 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 906 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 907 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 908 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 909 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 910 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 911 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 912 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 913 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 914 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 915 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 916 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 917 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 918 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 919 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 920 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 921 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 922 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 923 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 924 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 925 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 926 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 927 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 928 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 929 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 930 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 931 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 932 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 933 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 934 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 935 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 936 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 937 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 938 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 939 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 940 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 941 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 942 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 943 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 944 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 945 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 946 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 947 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 948 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 949 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 950 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 951 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 952 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 953 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 954 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 955 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 956 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 957 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 958 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 959 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 960 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 961 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 962 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 963 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 964 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 965 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 966 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 967 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 968 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 969 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 970 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 971 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 972 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 973 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 974 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 975 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 976 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 977 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 978 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 979 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 980 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 981 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 982 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 983 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 984 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 985 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 986 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 987 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 988 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 989 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 990 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 991 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 992 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 993 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 994 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 995 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 996 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 997 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n",
      "############ Epoch 998 ############\n",
      "Loss: 0.008932482451200485\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6951])\n",
      "\tBias: tensor([0.2993])\n",
      "############ Epoch 999 ############\n",
      "Loss: 0.0025885067880153656\n",
      "Updated parameters:\n",
      "\tWeights: tensor([0.6990])\n",
      "\tBias: tensor([0.3093])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# An epoch is one loop through the data...\n",
    "epochs = 1000\n",
    "\n",
    "# Rembember: \n",
    "# loss_fn = nn.L1Loss()\n",
    "# optimizer = torch.optim.SGD()\n",
    "\n",
    "# 0. Look through the data\n",
    "for epoch in range(epochs):\n",
    "    print(f\"############ Epoch {epoch} ############\")\n",
    "    # Set the model to training mode \n",
    "    model_0.train() # Train mode in PyTorch sets all parameters that requires gradients to require gradient \n",
    "    \n",
    "    # 1. Forward pass\n",
    "    y_pred = model_0(X_train) # Linear model\n",
    "    \n",
    "    # 2. Calculate loss: loss_fn(prediction, real)\n",
    "    loss = loss_fn(y_pred, y_train) # We are using MAE \n",
    "    print(f\"Loss: {loss}\")\n",
    "    \n",
    "    # 3. Optimizer \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step the optimizer (peform gradient descent)\n",
    "    optimizer.step() # By default how the optimizer changes will acumulate through the loop so we have to zero them at (3.) \n",
    "    print(f\"Updated parameters:\\n\\tWeights: {model_0.state_dict()['weights']}\\n\\tBias: {model_0.state_dict()['bias']}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "20a5bc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAJGCAYAAACTJvC6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdkElEQVR4nO3de3xU5b3v8e8KkFkgM0HADCxMg4n3SkFRKN4AJxUrm0JjC9WKSKs9WurWUuqGqlxsEdtjLVVQu6mKbU8rVsdLxYPWKXipuLEgtiqiMnKRMQNUzYzIBEie88c6icQkMBOSrJnJ5/16zWv6W1lr8huc0Hx5nvU8ljHGCAAAAADySIHXDQAAAABAWyPoAAAAAMg7BB0AAAAAeYegAwAAACDvEHQAAAAA5B2CDgAAAIC8Q9ABAAAAkHe6et1AOurq6hSLxeT3+2VZltftAAAAAPCIMUbJZFKO46igoOVxm5wIOrFYTCUlJV63AQAAACBLbNu2TUcffXSLX8+JoOP3+yW5byYQCHjcDQAAAACvJBIJlZSUNGSEluRE0KmfrhYIBAg6AAAAAA55SwuLEQAAAADIOwQdAAAAAHmHoAMAAAAg7xB0AAAAAOQdgg4AAACAvEPQAQAAAJB3cmJ56dbYt2+famtrvW4D8ES3bt3UpUsXr9sAAADwTN4FnUQioV27dqmmpsbrVgDPWJaloqIi9evX75BrzAMAAOSjvAo6iURC27dvV8+ePdW3b19169aNX/LQ6RhjtHv3bu3cuVPdu3dXr169vG4JAACgw+VV0Nm1a5d69uypo48+moCDTq179+6qqanRjh07VFRUxM8DAADodPJmMYJ9+/appqaGX+qA/y8QCKi2tpZ71QAAQKeUN0Gn/pe5bt26edwJkB26dnUHbPfv3+9xJwAAAB0vb4JOPUZzABc/CwAAoDPLu6ADAAAAABkHneeff17jxo2T4ziyLEuPPfbYIa9ZtWqVTjvtNPl8Ph177LFaunRpK1oFAAAAgPRkHHR2796twYMHa/HixWmd/95772ns2LEaPXq01q9fr+uuu05XXHGFnn766YybRXayLEujRo06rNdYtWqVLMvS3Llz26Sn9jZw4EANHDjQ6zYAAADQgoyXl/7qV7+qr371q2mff8899+iYY47RL3/5S0nSSSedpBdffFG/+tWvNGbMmEy/PVqQ6f0Yxph26gTpGjVqlJ577jn+WwAAALSDdt9HZ/Xq1aqoqGh0bMyYMbruuutavKampkY1NTUNdSKRaK/28sacOXOaHFu4cKGqq6ub/Vpb2rBhg3r06HFYrzFs2DBt2LBBffv2baOuAAAA0Jm1e9CpqqpSMBhsdCwYDCqRSGjPnj3q3r17k2sWLFigefPmtXdreaW5KV9Lly5VdXV1u08HO/HEEw/7NXr06NEmrwMAAABIWbrq2qxZs1RdXd3w2LZtm9ct5Y3NmzfLsixdfvnl2rBhg77+9a+rT58+sixLmzdvliQ9+uijuvjii3XssceqR48eKioq0jnnnKNHHnmk2dds7h6dyy+/XJZl6b333tMdd9yhE088UT6fT6WlpZo3b57q6uoand/SPTr198J88sknuvbaa+U4jnw+n770pS/p4YcfbvE9Tpo0Sb1791bPnj01cuRIPf/885o7d64sy9KqVavS/vN6/PHHdcYZZ6h79+4KBoO68sor9dFHHzV77ttvv63rr79ep512mvr06SPbtnX88cdr5syZ+uSTT5r8mT333HMN/7v+cfnllzecc99992n8+PEaOHCgbNtW7969NWbMGK1cuTLt/gEAAA5bKiVFo+5zDmn3EZ1+/fopHo83OhaPxxUIBJodzZEkn88nn8/X3q11au+++66+/OUva9CgQbr88sv173//W4WFhZLcoFlYWKizzz5b/fv3186dO/XEE0/oG9/4hu644w5dc801aX+fH//4x3ruuef0H//xHxozZowee+wxzZ07V3v37tX8+fPTeo19+/bp/PPP10cffaSLLrpIn376qR588EFNnDhRK1as0Pnnn99w7vbt23XmmWfqgw8+0AUXXKBTTz1VGzdu1Fe+8hWdd955Gf0Z/e53v9OUKVMUCAQ0efJk9erVS08++aQqKiq0d+/ehj+veuFwWPfee69Gjx6tUaNGqa6uTi+//LJ+/vOf67nnntPzzz/fsKHtnDlztHTpUm3ZsqXR1MIhQ4Y0/O9p06Zp8ODBqqio0FFHHaXt27frscceU0VFhcLhsMaPH5/R+wEAAMhYJCJVVkqJhBQISOGwFAp53VV6zGGQZB599NGDnnP99debU045pdGxiy++2IwZMybt71NdXW0kmerq6hbP2bNnj3nzzTfNnj170n7dfFdaWmo+/5/4vffeM5KMJDN79uxmr9u0aVOTY8lk0gwaNMgUFRWZ3bt3N/qaJDNy5MhGx6ZMmWIkmWOOOcbEYrGG4zt37jS9evUyfr/f1NTUNBxfuXKlkWTmzJnT7HsYP358o/OfffZZI6nJ5+jSSy81ksz8+fMbHb/33nsb3vfKlSubfd8Hqq6uNoFAwBxxxBFm48aNDcf37t1rzj33XCPJlJaWNrrm/fffb9RjvXnz5hlJ5g9/+EOj4yNHjmzy3+dA0Wi0ybFYLGYcxzHHHXfcId8DPxMAAOCw7NljTCBgjGUZI7nPgYB73EPpZANjjMl46tonn3yi9evXa/369ZLc5aPXr1+vrVu3SnJHAy677LKG86+66ipFo1Fdf/31euutt3TXXXfpoYce0g9/+MNWBbNskaMjeA369eunG264odmvlZWVNTnWs2dPXX755aqurtYrr7yS9ve56aab1L9//4a6b9++Gj9+vJLJpDZu3Jj26/zqV79qNIISCoVUWlraqJeamhr9+c9/VnFxsX70ox81un7q1Kk64YQT0v5+jz32mBKJhL7zne/o+OOPbzjerVu3FkeiBgwY0GSUR5J+8IMfSJKeffbZtL+/JB1zzDFNjvXv318XXXSR3nnnHW3ZsiWj1wMAAMhILOaO5NSvEGuMW8di3vaVpoyDzj/+8Q+deuqpOvXUUyVJ06dP16mnnqrZs2dLkj744IOG0CO5v6wtX75cf/3rXzV48GD98pe/1G9/+9ucXlo6EpGCQam83H2ORLzuKHODBw9u9pdySdqxY4emT5+uk046ST169Gi4f6Q+PMQy+HAPHTq0ybGjjz5akvTxxx+n9Rq9evVq9pf+o48+utFrbNy4UTU1NTr99NObTH20LEtnnnlm2n2/9tprkqRzzjmnyddGjBihrl2bzvo0xui+++7Tueeeq969e6tLly6yLEt9+vSRlNmfmyRFo1FdeeWVKi8vl23bDf8d7rzzzla9HgAAQEYcx52uVr+NiWW5teN421eaMr5HZ9SoUQfd92Pp0qXNXvPqq69m+q2yUirlTlNMJt06mXTreFyybW97y8TnV8Kr9+GHH+qMM87Q1q1bddZZZ6miokK9evVSly5dtH79ej3++OONlv4+lEAg0ORYfUiora1N6zWKioqaPd61a9dGixrUL0NeXFzc7PktvefmVFdXt/haXbp0aQgvB/rP//xPLVq0SCUlJfra176m/v37NwSuefPmZfTn9u6772rYsGFKJBIaPXq0xo0bp0AgoIKCAq1atUrPPfdcRq8HAACQMdt278mpv0fH73frHPmlt90XI8g39SN49Q4cwWtmxlfWammD0XvvvVdbt27VT3/6U914442Nvnbrrbfq8ccf74j2WqU+VO3YsaPZr39+UYyDqQ9Xzb1WbW2t/v3vf2vAgAENx3bs2KHFixfrS1/6klavXt1oX6GqqqqMl0v/1a9+pY8++ki///3vdemllzb62lVXXdWwYhsAAEC7CoWU2r5FO95Zr+Ljhsju2cvrjtKWlctLZ7McH8E7pE2bNklSsyt6vfDCCx3dTkZOOOEE+Xw+rV27tslohzFGq1evTvu1Bg8eLKn597x69Wrt37+/0bFoNCpjjCoqKppsntrSn1uXLl0kNT+y1dJ/B2OM/v73v6f5LgAAAA5PJBpRcFGpSp8YreCiUkWiuXPPBkEnQ/UjeH6/W+fYCN4hlZaWSpJefPHFRsf/+Mc/6qmnnvKipbT5fD594xvfUDwe18KFCxt97Xe/+53eeuuttF9r/PjxCgQCuu+++/T22283HN+3b1+TkS7psz+3l156qdF0uvfff1+zZs1q9nv07t1bkprdJ6ql/w633nqrXn/99bTfBwAAQGul9qdU+VClkjXuPRvJmqQqH6pUan9urMbF1LVWCIXce3JiMXckJ19CjiRNnjxZP//5z3XNNddo5cqVKi0t1WuvvaZIJKLKykqFw2GvWzyoBQsW6Nlnn9XMmTP13HPPNeyj8+STT+qCCy7QihUrVFBw6HxfVFSkO+64Q5dffrnOOOMMfetb31JRUZGefPJJde/evdFKctJnq6E98sgjOv300xUKhRSPx/Xkk08qFAo1jNAc6LzzztPDDz+siy66SF/96ldl27YGDx6scePG6aqrrtL999+viy66SBMnTlSfPn308ssva926dRo7dqyWL1/eZn9mAAAAzYklY0rUfHbPhpFRoiahWDKmsiOz/54NRnRaybbde3LyKeRI7kpmzz33nEKhkJ599ln95je/0d69e/XMM89o3LhxXrd3SCUlJVq9erW++c1v6qWXXtLChQu1Y8cOPfPMMzr22GMlNb9AQnOmTJmiRx99VMcdd5weeOABPfDAAzrrrLP07LPPNrti3dKlS/WjH/1IH330ke688069/PLLmj59uv74xz82+/pXXnmlrr/+eu3atUs///nPddNNN+mRRx6RJJ166ql65plndNpppykcDuu+++5Tr1699Pe//12nn356K/90AABAp5bh/iiO31HAF5Al954NS5YCvoAcf27cs2GZgy2hliUSiYSKiopUXV3d4i+pqVRK7733no455hjZ+ZY+0CbOPvtsrV69WtXV1erZs6fX7bQ7fiYAAECDSOSz1dMCAffei1Do0JdFI6p8qFKJmoQCvoDCE8MKlR36uvaUTjaQmLqGPPTBBx80mVr2hz/8QX//+991/vnnd4qQAwAA0OAw9kcJlYUUnxFXLBmT43dkd82dfzwl6CDvnHLKKTr11FN18sknN+z/s2rVKvn9ft12221etwcAANCxDnN/FLurnRP35HweQQd556qrrtJf/vIX/eMf/9Du3bt11FFH6ZJLLtFNN92kE0880ev2AAAAOlb9/ijJpBtyLMtdOjhf9kdpAUEHeWf+/PmaP3++120AAABkh/r9Uerv0cm3/VFaQNABAAAA8l0opNT2LdrxznoVHzdEds9eXnfU7lheGgAAAMhzkWhEwUWlKn1itIKLShWJRrxuqd0RdAAAAIA8ltqfUuVDlUrWuKuuJWuSqnyoUqn96e2nk6sIOgAAAEAeiyVjStQkZORun2lklKhJKJaMedxZ+yLoAAAAALkklZKiUfc5DY7fUcAXkCVLkmTJUsAXkOPP71XXCDoAAABArohEpGBQKi93nyOHvtfG7morPDEsv88vSfL7/ApPDOfU5p+twaprAAAAQC5IpdwlopPuvTZKJt06Hj/kUtGhspDiM+KKJWNy/E7ehxyJER0AAAAgN8Ri7j44xr3XRsa4dSy9e23srrbKjizrFCFHIugAAAAAucFxpEBAstx7bWRZbu3k9702rUXQQdaaO3euLMvSqlWrvG4FAADAe7YthcMyfvdeG+P3S+HwIaetdVYEnTxhWVZGj7aWraFk6dKlsixLS5cu9boVAACAwxY5RgrOMCr7T/c5cozXHWUvFiPIE3PmzGlybOHChaqurm72awAAAMgtDRt/1n2inb0lq+4TVT5UqfiMeKe57yYTBJ08MXfu3CbHli5dqurq6ma/BgAAgNxSv/FnvQM3/iw7sszDzrITU9c6ob179+r222/XaaedpiOOOEJ+v1/nnHOOnnjiiSbnVldXa/bs2Tr55JPVs2dPBQIBHXvssZoyZYq2bNkiSRo1apTmzZsnSRo9enTD9LiBAwem1c+2bdt08cUXq3fv3urZs6dGjhyp559/vsXe77zzTo0ZM0YlJSXy+XwqLi5WZWWlXn311UbnXn755Zo6daokaerUqc1O3Vu7dq1+8IMf6JRTTlFRUZG6d++uQYMG6dZbb9W+ffvS6h8AACBjGW76KXXejT9bixGdTqampkYXXHCBVq1apSFDhui73/2u9u3bp+XLl2v8+PG688479YMf/ECSZIzRmDFj9D//8z8666yzdMEFF6igoEBbtmzRE088ocmTJ6u0tFSXX365JOm5557TlClTGgJOr169DtnPBx98oBEjRmj79u0aM2aMTjvtNG3YsEFf+cpXNHr06Cbnf/jhh7ruuut0zjnn6MILL9SRRx6paDSqJ554Qv/3//5fPf/88zrjjDMkSRMmTNDHH3+sxx9/XOPHj9eQIUOavN6SJUv0l7/8Reeee64uvPBCffrpp1q1apVmzZqlV155RY888kir/pwBAABaFIm4+98kEu6qaeGwFAod8rL6jT8rH6pUoibRaTb+bDWTA6qrq40kU11d3eI5e/bsMW+++abZs2dPh/S0Z98es+nDTWbPvo75fq1RWlpqPv+f+Cc/+YmRZG666SZTV1fXcDyRSJjTTz/dFBYWmu3btxtjjPnnP/9pJJkJEyY0ee1UKmWSyWRDPWfOHCPJrFy5MqMep0yZYiSZn/3sZ42O/+Y3vzGSmrxmKpUy77//fpPXef31103Pnj1NRUVFo+P333+/kWTuv//+Zr//li1bzP79+xsdq6urM9/5zneMJPPiiy9m9H6ySUf/TAAAgDTs2WNMIGCMZRkjuc+BgHs83ZfIgd9D21M62cAYY5i61gqRaETB24Iqv6NcwduCikQjXreUlrq6Ot19990qLy/XvHnzGk3h8vv9mj17tvbu3atwONzouu7duzd5LZ/Pp549ex5WP3v37tWyZctUXFysH/3oR42+dsUVV+i4445r9vsOGDCgyfEvfvGLGj16tJ5//vmMppx94QtfUJcuXRodsyxL06ZNkyQ9++yzab8WAADAIR3mpp9S59v4s7WYupahhtUuapKSpGRNMmdWu9i4caM++ugjOY7TcE/NgXbu3ClJeuuttyRJJ510kr70pS/pT3/6k95//31NmDBBo0aN0pAhQ1RQcPgZeePGjUqlUjrvvPNkf27994KCAp111ll65513mly3fv16/eIXv9CLL76oqqqqJsFm165d6t+/f1o97N27V4sWLdKDDz6ot956S5988olM/V88kmIZ/KUDAABwSPWbfiaTbsixLMnvZ9PPdkDQyVAur3bx4YcfSpLeeOMNvfHGGy2et3v3bklS165d9be//U1z587VI4880jDqctRRR+kHP/iBbrjhhiajIZmorq6WJBUXFzf79WAw2OTYSy+9pPPOO0+SdP755+u4445Tz549ZVmWHnvsMb322muqqalJu4dvfOMb+stf/qLjjz9ekyZNUnFxsbp166aPP/5Yv/71rzN6LQAAgEP6/5t+Ntyjw6af7Yagk6H61S6SNUkZGVmy5Pf5c2K1i0AgIEm66KKL9PDDD6d1TZ8+fXTnnXfqjjvu0FtvvaW//e1vuvPOOzVnzhx169ZNs2bNanU/RUVFkqQdO3Y0+/V4PN7k2Pz581VTU6MXXnhBZ599dqOvvfzyy3rttdfS/v6vvPKK/vKXv2jMmDFavnx5o9D28ssv69e//nXarwUAAJC2UEip7Vu04531Kj5uiOyevbzuKC9xj06G6le78Pv8kpRTq12cdNJJCgQC+sc//pHx0smWZemkk07StGnT9Ne//lWSGi1HXR8Samtr037N448/XrZt6x//+IdSn1tasa6uTi+99FKTazZt2qTevXs3CTmffvqp1q1b1+T8g/W1adMmSdLYsWObjEy98MILab8PAACATESiEQUXlar0idEKLirNmfu9cw1BpxVCZSHFZ8S16T83KT4jrlDZoZcDzAZdu3bV1VdfrS1btmjGjBnNhp3XX3+9YYRl8+bN2rx5c5Nz6kdaDryvpnfv3pLcPXHS5fP5NHHiRO3YsUO//OUvG33tt7/9rd5+++0m15SWluqjjz5qNPWutrZWM2bMaLjH6EAH66u0tFSS9OKLLzY6/sYbb2jBggVpvw8AAIB0tXS/d2p/+vvpID1MXWul+tUucs28efO0bt063XHHHVq+fLnOPfdcFRcXa/v27frXv/6l1157TatXr1ZxcbHWr1+vyspKDRs2TCeffLL69eun7du367HHHlNBQYF++MMfNrxu/UahP/nJT/TGG2+oqKhIvXr1atiTpyW33nqrIpGIbrzxRr344os69dRTtWHDBj311FM6//zz9cwzzzQ6/5prrtEzzzyjs88+WxMnTpRt21q1apW2b9+uUaNGadWqVY3OHzFihLp3766FCxfqo48+0lFHHSVJuvHGGzVs2DANGzZMDz30kD744AN9+ctf1tatW/XEE09o7NixaU/vAwAASFcu3++dczpmtevDk4376OSC5vbRMcaY/fv3m9/85jfmrLPOMoFAwPh8PvOFL3zBXHDBBebuu+82n3zyiTHGmG3btpmZM2eaL3/5y6a4uNgUFhaaL3zhC6aystKsXr26yesuXbrUDBo0yPh8PiPJlJaWptXnli1bzKRJk0yvXr1Mjx49zDnnnGOee+65Fvfmefjhh81pp51mevToYfr27WsmTpxoNm3a1LAnz3vvvdfo/OXLl5szzjjDdO/evWFvnno7duww3/nOd4zjOMa2bTNo0CCzePFiE41GjSQzZcqUtN5DNuJnAgCADrBnjzGbNqW9D86efXtMYEHAWHMto7ky1lzLBBYEOu2eOK2R7j46ljEHrKWbpRKJhIqKilRdXd1wQ/3npVIpvffeezrmmGOaLFUMdEb8TAAA0M4ikc9WTwsE3NXTQoe+pSESjajyoUolahIK+AIKTwznzK0Q2SCdbCAxdQ0AAADIXCrlhpyke6+Nkkm3jscPuVR0/f3esWRMjt/JiUWtchGLEQAAAACZisXckZz6yVHGuHWam43X3+9NyGk/BB0AAAAgU47jTlezLLe2LLd2sn9vxc6CoAMAAABkyrbde3L87t6K8vvdmvtiswb36AAAAACtEQoptX2LdryzXsXHDZHds5fXHeEAjOgAAAAArRCJRhRcVKrSJ0YruKhUkWjE65ZwAIIOAAAAkKHU/pQqH6pUssZddS1Zk1TlQ5VK7U953BnqEXQAAACADMWSMSVqEjJyV10zMkrUJBRLprfqGtofQQcAAABIpaRo1H1Og+N3FPAFZMlddc2SpYAvIMfPqmvZgqADAACAzi0SkYJBqbzcfY4c+l4bu6ut8MSw/D531TW/z6/wxDD74mQRVl0DAABA55VKSZWVUtK910bJpFvH44dcKjpUFlJ8RlyxZEyO3yHkZBlGdAAAANB5xWJSIiEZ914bGePWsfTutbG72io7soyQk4UIOgAAAOi8HEcKBCTLvddGluXWDvfa5DqCDtrd5s2bZVmWLr/88kbHR40aJav+L5V2MHDgQA0cOLDdXh8AAOQB25bCYcnv3msjv9+tDzFtDdmPoJNn6kPFgY/CwkKVlJTokksu0T//+U+vW2wzl19+uSzL0ubNm71uBQAA5LJQSKntW7R13Uqltm+RQiGvO0IbYDGCPFVeXq5LL71UkvTJJ5/o5Zdf1p/+9CeFw2FFIhGdddZZHnco/e53v9Onn37abq8fSWPFFAAAgEg0osqHKpWoSSjgCyg8MaxQGWEn1xF08tSxxx6ruXPnNjp24403av78+brhhhu0atUqT/o60Be+8IV2ff3y8vJ2fX0AAJD7UvtTqnyoUskad9W1ZE1SlQ9VKj4jzgIDOY6pa53INddcI0l65ZVXJEmWZWnUqFHavn27LrvsMvXr108FBQWNQtDzzz+vcePGqW/fvvL5fDruuON04403NjsSU1tbq5///Oc69thjZdu2jj32WC1YsEB1dXXN9nOwe3Qef/xxnX/++erTp49s29bAgQM1efJkvf7665Lc+28eeOABSdIxxxzTME1v1KhRDa/R0j06u3fv1pw5c3TiiSfKtm317t1bY8eO1d///vcm586dO1eWZWnVqlX64x//qCFDhqh79+7q37+/rr32Wu3Zs6fJNY888ohGjhyp4uJi2bYtx3FUUVGhRx55pNn3CgAAvBNLxpSoScjIXXXNyChRk1Asmd6qa8hejOh0QgeGi3//+98aMWKEevfurW9961tKpVIKBAKSpLvvvlvTpk1Tr169NG7cOBUXF+sf//iH5s+fr5UrV2rlypUqLCxseK3vfe97uu+++3TMMcdo2rRpSqVSuv322/XSSy9l1N+PfvQj3X777erdu7cmTJig4uJibdu2Tc8++6yGDh2qU045Rdddd52WLl2q1157Tddee6169eolSYdcfCCVSum8887TmjVrdNppp+m6665TPB7XsmXL9PTTT+tPf/qTvvnNbza5btGiRVqxYoXGjx+v8847TytWrNAdd9yhXbt26f/8n//TcN7dd9+t73//++rfv7++/vWvq0+fPqqqqtKaNWv06KOP6qKLLsrozwIAAGQglXKXhXactBcTcPyOAr6AkjVJGRlZsuT3+eX4WXUt55kcUF1dbSSZ6urqFs/Zs2ePefPNN82ePXs6pqk9e4zZtMl9ziLvvfeekWTGjBnT5GuzZ882kszo0aONMcZIMpLM1KlTzf79+xud+8Ybb5iuXbuawYMHm127djX62oIFC4wkc9tttzUcW7lypZFkBg8ebD755JOG4++//77p27evkWSmTJnS6HVGjhxpPv8R/Mtf/mIkmUGDBjX5vvv27TNVVVUN9ZQpU4wk89577zX7Z1FaWmpKS0sbHZs3b56RZL797W+burq6huPr1q0zhYWFplevXiaRSDQcnzNnjpFkioqKzFtvvdVw/NNPPzXHH3+8KSgoMNu3b284ftppp5nCwkITj8eb9PP599PeOvxnAgAALz37rDGBgDGS+/zss+lfuulZE1gQMJorE1gQMM9uSv9adLx0soExxjB1rTUiESkYlMrL3ecsvOn93Xff1dy5czV37lz9+Mc/1rnnnqubb75Ztm1r/vz5DecVFhbqF7/4hbp06dLo+t/85jfav3+/7rzzTvXp06fR166//nodddRR+tOf/tRw7He/+50kafbs2TriiCMajg8YMEDXXntt2n3fddddkqRf//rXTb5v165dFQwG036t5jzwwAPq1q2bbr311kYjW6eeeqqmTJmijz/+WI899liT66699lqdcMIJDXX37t118cUXq66uTmvXrm10brdu3dStW7cmr/H59wMAANpIKiVVVkpJ9z4bJZNunUqldXmoLKT4jLg2/ecmxWfEWYggTzB1LVMt/SDF41m13vqmTZs0b948Se4v3sFgUJdccolmzpypQYMGNZx3zDHHqG/fvk2uf/nllyVJTz/9dLOrl3Xr1k1vvfVWQ/3aa69Jks4555wm5zZ3rCVr1qyRz+fTyJEj074mXYlEQtFoVCeddJKOPvroJl8fPXq0lixZovXr12vy5MmNvjZ06NAm59e/xscff9xw7Fvf+pauv/56nXLKKbrkkks0evRonX322Q3TAQEAQDuIxaRE4rPaGLeOxaSysrRewu5qq+zI9M5FbiDoZKoNfpA6wpgxY7RixYpDntfSCMmHH34oSY1Gfw6murpaBQUFzYamTEZhqqurNWDAABUUtP1gY+L//3drqZ/+/fs3Ou9AzQWVrl3dH5/a2tqGYzNmzFCfPn10991365e//KVuu+02de3aVWPHjtWvfvUrHXPMMYf9PgAAwOc4jhQIuP8AbYxkWe7Gnw732XRmTF3LVP0PUv20J8ty6xz9QWpp1bP6X+wTiYSMMS0+6hUVFamurk67du1q8lrxeDztfnr16qWqqqoWV2o7HPXvqaV+qqqqGp3XGpZl6Tvf+Y5eeeUV7dy5U48++qgqKyv1+OOP6z/+4z8ahSIAANBGbFsKh91wI7nP4XBWzbZBxyPoZKqT/CANHz5c0mdT2A5l8ODBkqQXXnihydeaO9aSYcOGqaamRs8999whz62/ryjd8BAIBFRWVqZ3331X27dvb/L1+mW1hwwZkna/B9OnTx9NmDBBy5Yt03nnnac333xT7777bpu8NgAA+JxQSKntW7R13Uqltm+RQtxn09m1KugsXrxYAwcOlG3bGj58uNasWdPiufv27dPNN9+s8vJy2batwYMHpzWlKquFQu49OZs2uc95+IP0/e9/X127dtU111yjrVu3Nvn6xx9/rFdffbWhrr+n5eabb9bu3bsbjm/fvl2//vWv0/6+06ZNk+Te/F8/fa7e/v37G43G9O7dW5K0bdu2tF9/ypQp2rdvn2bNmtVoROqf//ynli5dqqKiIk2YMCHt1/u8VatWNXpdyf0ZqH8vdp4FYgAAskUkGlFwUalKnxit4KJSRaLZt1gUOlbG9+gsW7ZM06dP1z333KPhw4dr4cKFGjNmjDZu3Kji4uIm59944436wx/+oCVLlujEE0/U008/ra9//et66aWXdOqpp7bJm/CEbWfVPTlt7ZRTTtFdd92lq6++WieccIIuvPBClZeXK5lMKhqN6rnnntPll1+ue+65R5J7I//UqVN1//33a9CgQfr617+umpoaLVu2TF/+8pf15JNPpvV9L7zwQs2YMUO33XabjjvuOH39619XcXGxtm/frkgkohkzZui6666TJJ133nm67bbb9L3vfU8XXXSRjjjiCJWWljZZSOBA119/vZYvX67f//732rBhg0KhkHbs2KFly5Zp//79WrJkifz1o3WtMGHCBAUCAX35y19WaWmp9u3bp7/+9a9688039Y1vfEOlpaWtfm0AANC81P6UKh+qVLLGXSwqWZNU5UOVis+Iy+7KPzJ2WpmuWz1s2DAzbdq0hrq2ttY4jmMWLFjQ7Pn9+/c3ixYtanSssrLSfPvb327xe6RSKVNdXd3w2LZtW/bto5OlDraPzudJMiNHjjzoOWvWrDHf+ta3jOM4plu3bqZv377mtNNOMzNnzjQbNmxodO7+/fvNggULTFlZmSksLDRlZWXmlltuMe+++27a++jUe+SRR8zo0aNNUVGR8fl8ZuDAgWby5Mnm9ddfb3TeL37xC3PccceZbt26NXk/ze2jY4wxn3zyibnpppvM8ccf37B3zle/+lXzwgsvNDm3fh+dlStXNvna/fffbySZ+++/v+HYXXfdZb72ta+Z0tJSY9u26dOnjxk2bJi5++67zd69e5t9r+2FnwkAQGex6cNNRnPV5LHpw01et4Z2kO4+OpYxn5tncxB79+5Vjx499PDDDzea3lO//8jjjz/e5Jo+ffroF7/4hb773e82HLv00kv14osvavPmzc1+n7lz5zYsjXyg6urqFm8UT6VSeu+993TMMccwPQgQPxMAgByWSrkr2jpOWvdBp/anFLwtqGRNUkZGliz5fX5GdPJUIpFQUVHRQbOBlOE9Ort27VJtbW2T5XmDwWDDilWfN2bMGN1+++165513VFdXp7/+9a8Kh8P64IMPWvw+s2bNUnV1dcMjk3swAAAAkMNasTG73dVWeGJYfp87/dzv8ys8MUzI6eTafR+dX//617ryyit14oknyrIslZeXa+rUqbrvvvtavMbn88nn87V3awAAAMgmh7Exe6gspPiMuGLJmBy/Q8hBZiM6ffv2VZcuXZrsQxKPx9WvX79mrznqqKP02GOPaffu3dqyZYveeust9ezZU2V5fCM/AAAAWqF+Y/b6OysO3Jg9DXZXW2VHlhFyICnDoFNYWKihQ4cqcsAQYl1dnSKRiEaMGHHQa23b1oABA7R//3498sgjGj9+fOs6BgAAQH7Ks43Z4a2M99GZPn26lixZogceeEAbNmzQ1Vdfrd27d2vq1KmSpMsuu0yzZs1qOP9//ud/FA6HFY1G9cILL+iCCy5QXV2drr/++rZ7FwAAAMh9nWRjdnSMjO/RmTRpknbu3KnZs2erqqpKQ4YM0YoVKxoWKNi6dasKCj7LT6lUSjfeeKOi0ah69uypCy+8UL///e/Vq1evNnsTAAAAyBOhkFLbt2jHO+tVfNwQ2T17ed0RclRGy0t7JZ0l5OqX0h04cKC6d+/ewR0C2WfPnj3avHkzy0sDAHJKJBpR5UOVStQkFPAFFJ4YVqgs5HVbyCLtsrx0NuvSpYskad++fR53AmSH/fv3S5K6dm33xRUBAGgTqf0pVT5UqWSNu+pasiapyocqldqf8rgz5KK8CTrdunWTz+dTdXW1cmCQCmh3iURCXbp0afhHAAAAsl0sGVOiJiEj93c5I6NETUKxZHqrrgEHyqt/6u3bt6+2b9+u999/X0VFRerWrZus+lU7gE7CGKPdu3crkUiof//+/AwAALyRSrnLQjtO2osJOH5HAV9AyZqkjIwsWfL7/HL8rLqGzOVV0Kmfo7dr1y5t377d424A71iWpV69eqmoqMjrVgAAnVEk4m70mUi4y0OHw1Lo0PfZ2F1thSeGG+7R8fv8Ck8Msy8OWiVvFiP4vH379qm2tradOwOyU7du3ZiyBgDwRiolBYNSMulu+GlZ7jLR8XjaIzup/SnFkjE5foeQgybSzQZ5NaJzoG7duqlbt25etwEAANC5xGLuSE49Y9w6FpPKytJ6CburrbIj0zsXaEneLEYAAACALOA47nS1+ntELcutHe6zQcci6AAAAKDt2LZ7T47f79Z+v1uzpxs6WN5OXQMAAIBHQiGltm/RjnfWq/i4IbJ79vK6I3RCjOgAAACgTUWiEQUXlar0idEKLipVJBrxuiV0QgQdAAAAtJnU/pQqH6pUsiYpSUrWJFX5UKVS+1Med4bOhqADAACANhNLxpSoScjI3cHEyChRk1AsGfO4M3Q2BB0AAAC0LJWSolH3OQ2O31HAF5Ald9U1S5YCvoAcP6uuoWMRdAAAANC8SMTd/LO83H2OHPpeG7urrfDEsPw+d9U1v8+v8MQwG3+iw1nGGON1E4eS7u6nAAAAaCOplBtukkl300/LcpeKjsfTWio6tT+lWDImx+8QctCm0s0GjOgAAACgqVhMSiTckCO5z4mEezwNdldbZUeWEXLgGYIOAAAAmnIcKRBwR3Ik9zkQcI8DOYCgAwAAgKZsWwqH3elqkvscDqc1bQ3IBl29bgAAAABZKhRSavsW7XhnvYqPGyK7Zy+vOwLSxogOAAAAmhWJRhRcVKrSJ0YruKhUkeihV10DsgVBBwAAAE2k9qdU+VClkjVJSVKyJqnKhyqV2p/efjqA1wg6AAAAaCKWjClRk5CRu+qakVGiJqFYMr1V1wCvEXQAAADQhON3FPAFZMlddc2SpYAvIMfPqmvIDQQdAAAANGF3tRWeGJbf56665vf5FZ4YZl8c5AxWXQMAAECzQmUhxWfEFUvG5PgdQg5yCkEHAAAALbK72io7sszrNoCMMXUNAAAAQN4h6AAAAHQCqZQUjbrPQGdA0AEAAMhzkYgUDErl5e5zhH0/0QkQdAAAAPJYKiVVVkpJd99PJZNuzcgO8h1BBwAAII/FYlIiIRl3308Z49Yx9v1EniPoAAAA5DHHkQIByXL3/ZRlubXDvp/IcwQdAACAPGbbUjgs+d19P+X3u7XNljjIc+yjAwAAkOdCISked6erOQ4hB50DQQcAAKATsG2pjH0/0YkwdQ0AAABA3iHoAAAAAMg7BB0AAIAckUpJ0Sh74ADpIOgAAADkgEhECgal8nL3ORLxuiMguxF0AAAAslwqJVVWSsmkWyeTbs3IDtAygg4AAECWi8WkREIyxq2NcetYzNu+gGxG0AEAAMhyjiMFApJlubVlubXjeNsXkM0IOgAAAFnOtqVwWPL73drvd2s2/gRaxoahAAAAOSAUkuJxd7qa4xBygEMh6AAAAOQI25bKyrzuAsgNTF0DAAAAkHcIOgAAAADyDkEHAACgg6VSUjTKPjhAeyLoAAAAdKBIRAoGpfJy9zkS8bojID8RdAAAADpIKiVVVkrJpFsnk27NyA7Q9gg6AAAAHSQWkxIJyRi3NsatYzFv+wLyEUEHAACggziOFAhIluXWluXWjuNtX0A+IugAAAB0ENuWwmHJ73drv9+t2fwTaHtsGAoAANCBQiEpHnenqzkOIQdoLwQdAACADmbbUlmZ110A+Y2pawAAAADyDkEHAAAAQN4h6AAAALRSKiVFo+yDA2Qjgg4AAEArRCJSMCiVl7vPkYjXHQE4UKuCzuLFizVw4EDZtq3hw4drzZo1Bz1/4cKFOuGEE9S9e3eVlJTohz/8oVL80wcAAMhRqZRUWSklk26dTLo1v94A2SPjoLNs2TJNnz5dc+bM0bp16zR48GCNGTNGO3bsaPb8P/7xj5o5c6bmzJmjDRs26N5779WyZcv0k5/85LCbBwAA8EIsJiUSkjFubYxbx2Le9gXgMxkHndtvv11XXnmlpk6dqpNPPln33HOPevToofvuu6/Z81966SWdddZZuuSSSzRw4ECdf/75uvjiiw85CgQAAJCtHEcKBCTLcmvLcmvH8bYvAJ/JKOjs3btXa9euVUVFxWcvUFCgiooKrV69utlrzjzzTK1du7Yh2ESjUT311FO68MILW/w+NTU1SiQSjR4AAADZwralcFjy+93a73drNv8EskdGG4bu2rVLtbW1CgaDjY4Hg0G99dZbzV5zySWXaNeuXTr77LNljNH+/ft11VVXHXTq2oIFCzRv3rxMWgMAAOhQoZAUj7vT1RyHkANkm3ZfdW3VqlW65ZZbdNddd2ndunUKh8Navny5fvrTn7Z4zaxZs1RdXd3w2LZtW3u3CQAAkDHblsrKCDlANspoRKdv377q0qWL4vF4o+PxeFz9+vVr9pqbbrpJkydP1hVXXCFJGjRokHbv3q3vfe97uuGGG1RQ0DRr+Xw++Xy+TFoDAAAAgAYZjegUFhZq6NChihywUHxdXZ0ikYhGjBjR7DWffvppkzDTpUsXSZKpX6oEAAAAANpQRiM6kjR9+nRNmTJFp59+uoYNG6aFCxdq9+7dmjp1qiTpsssu04ABA7RgwQJJ0rhx43T77bfr1FNP1fDhw/Xuu+/qpptu0rhx4xoCDwAAgFdSKe6zAfJRxkFn0qRJ2rlzp2bPnq2qqioNGTJEK1asaFigYOvWrY1GcG688UZZlqUbb7xR27dv11FHHaVx48Zp/vz5bfcuAAAAWiEScTf6TCTc5aHDYXeRAQC5zzI5MH8skUioqKhI1dXVCgQCXrcDAADyQColBYNSMulu+GlZ7jLR8TgjO0A2SzcbtPuqawAAANkoFnNHcur/ydcYt47FvO0LQNsg6AAAgE7Jcdzpapbl1pbl1o7jbV8A2gZBBwAAdEq27d6T4/e7td/v1kxbA/JDxosRAAAA5ItQyL0nh1XXgPxD0AEAAJ2abUtlZV53AaCtMXUNAAAAQN4h6AAAAADIOwQdAACQF1IpKRp1nwGAoAMAAHJeJOJu/lle7j5HIl53BMBrBB0AAJDTUimpslJKJt06mXRrRnaAzo2gAwAAclosJiUSkjFubYxbx2Le9gXAWwQdAACQ0xxHCgQky3Jry3Jrx/G2LwDeIugAAICcZttSOCz5/W7t97s1m38CnRsbhgIAgJwXCknxuDtdzXEIOQAIOgAAIE/YtlRW5nUXALIFU9cAAAAA5B2CDgAAAIC8Q9ABAABZI5WSolH2wAFw+Ag6AAAgK0QiUjAolZe7z5GI1x0ByGUEHQAA4LlUSqqslJJJt04m3ZqRHQCtRdABAACei8WkREIyxq2NcetYzNu+AOQugg4AAPCc40iBgGRZbm1Zbu043vYFIHcRdAAAgOdsWwqHJb/frf1+t2bjTwCtxYahAAAgK4RCUjzuTldzHEIOgMND0AEAAFnDtqWyMq+7AJAPmLoGAAAAIO8QdAAAAADkHYIOAABoc6mUFI2yDw4A7xB0AABAm4pEpGBQKi93nyMRrzsC0BkRdAAAQJtJpaTKSimZdOtk0q0Z2QHQ0Qg6AACgzcRiUiIhGePWxrh1LOZtXwA6H4IOAABoM44jBQKSZbm1Zbm143jbF4DOh6ADAADajG1L4bDk97u13+/WbP4JoKOxYSgAAGhToZAUj7vT1RyHkAPAGwQdAADQ5mxbKivzugsAnRlT1wAAAADkHYIOAAAAgLxD0AEAAC1KpaRolH1wAOQegg4AAGhWJCIFg1J5ufsciXjdEQCkj6ADAACaSKWkykopmXTrZNKtGdkBkCsIOgAAoIlYTEokJGPc2hi3jsW87QsA0kXQAQAATTiOFAhIluXWluXWjuNtXwCQLoIOAABowralcFjy+93a73drNv8EkCvYMBQAADQrFJLicXe6muMQcgDkFoIOAABokW1LZWVedwEAmWPqGgAAAIC8Q9ABACDPsekngM6IoAMAQB5j008AnRVBBwCAPMWmnwA6M4IOAAB5ik0/AXRmBB0AAPIUm34C6MwIOgAA5Ck2/QTQmbGPDgAAeYxNPwF0VgQdAADyHJt+AuiMmLoGAAAAIO8QdAAAAADkHYIOAAA5IpWSolH2wQGAdBB0AADIAZGIFAxK5eXucyTidUcAkN0IOgAAZLlUSqqslJJJt04m3ZqRHQBoGUEHAIAsF4tJiYRkjFsb49axmLd9AUA2a1XQWbx4sQYOHCjbtjV8+HCtWbOmxXNHjRoly7KaPMaOHdvqpgEA6EwcRwoEJMtya8tya8fxti8AyGYZB51ly5Zp+vTpmjNnjtatW6fBgwdrzJgx2rFjR7Pnh8NhffDBBw2P119/XV26dNE3v/nNw24eAIDOwLalcFjy+93a73drNv8EgJZZxtQPhKdn+PDhOuOMM7Ro0SJJUl1dnUpKSnTNNddo5syZh7x+4cKFmj17tj744AMdccQRaX3PRCKhoqIiVVdXKxAIZNIuAAB5I5Vyp6s5DiEHQOeVbjbIaERn7969Wrt2rSoqKj57gYICVVRUaPXq1Wm9xr333qtvfetbBw05NTU1SiQSjR4AAHR2ti2VlRFyACAdGQWdXbt2qba2VsFgsNHxYDCoqqqqQ16/Zs0avf7667riiisOet6CBQtUVFTU8CgpKcmkTQAAAACdXIeuunbvvfdq0KBBGjZs2EHPmzVrlqqrqxse27Zt66AOAQAAAOSDrpmc3LdvX3Xp0kXxeLzR8Xg8rn79+h302t27d+vBBx/UzTfffMjv4/P55PP5MmkNAICcwb02AND+MhrRKSws1NChQxU5YDvmuro6RSIRjRgx4qDX/vnPf1ZNTY0uvfTS1nUKAEAeiESkYFAqL3efD/i/VABAG8p46tr06dO1ZMkSPfDAA9qwYYOuvvpq7d69W1OnTpUkXXbZZZo1a1aT6+69915NmDBBffr0OfyuAQDIQamUVFkpJZNunUy6dSrlbV8AkI8ymromSZMmTdLOnTs1e/ZsVVVVaciQIVqxYkXDAgVbt25VQUHj/LRx40a9+OKLeuaZZ9qmawAAclAsJh24kKgxbh2LuaupAQDaTsb76HiBfXQAAPkglXKnqyWTbsixLHfzz3ice3UAIF3tso8OAABoPduWwmE33EjuczhMyAGA9pDx1DUAANB6oZA7gsOqawDQvgg6AAB0MNvmnhwAaG9MXQMAAACQdwg6AAAAAPIOQQcAgFZIpaRolD1wACBbEXQAAMhQJOIuE11e7j5HIl53BAD4PIIOAAAZSKWkykp3LxzJfa6sZGQHALINQQcAgAzEYlIi4W74KbnPiYR7HACQPQg6AABkwHGkQECyLLe2LLd2HG/7AgA0RtABACADti2Fw5Lf79Z+v1uz8ScAZBc2DAUAIEOhkBSPu9PVHIeQAwDZiKADAEAr2LZUVuZ1FwCAljB1DQAAAEDeIegAAAAAyDsEHQBAp5ZKSdEo++AAQL4h6AAAOq1IRAoGpfJy9zkS8bojAEBbIegAADqlVEqqrJSSSbdOJt2akR0AyA8EHQBApxSLSYmEZIxbG+PWsZi3fQEA2gZBBwDQKTmOFAhIluXWluXWjuNtXwCAtkHQAQB0SrYthcOS3+/Wfr9bs/knAOQHNgwFAHRaoZAUj7vT1RyHkAMA+YSgAwDo1GxbKivzugsAQFtj6hoAAACAvEPQAQAAAJB3CDoAgJyXSknRKHvgAAA+Q9ABAOS0SEQKBqXycvc5EvG6IwBANiDoAAByViolVVZKyaRbJ5NuzcgOAICgAwDIWbGYlEhIxri1MW4di3nbFwDAewQdAEDOchwpEJAsy60ty60dx9u+AADeI+gAAHKWbUvhsOT3u7Xf79Zs/AkAYMNQAEBOC4WkeNydruY4hBwAgIugAwDIebYtlZV53QUAIJswdQ0AAABA3iHoAAAAAMg7BB0AQNZIpaRolH1wAACHj6ADAMgKkYgUDErl5e5zJOJ1RwCAXEbQAQB4LpWSKiulZNKtk0m3ZmQHANBaBB0AgOdiMSmRkIxxa2PcOhbzti8AQO4i6AAAPOc4UiAgWZZbW5ZbO463fQEAchdBBwDgOduWwmHJ73drv9+t2fwTANBabBgKAMgKoZAUj7vT1RyHkAMAODwEHQBA1rBtqazM6y4AAPmAqWsAAAAA8g5BBwAAAEDeIegAANpcKiVFo+yDAwDwDkEHANCmIhEpGJTKy93nSMTrjgAAnRFBBwDQZlIpqbJSSibdOpl0a0Z2AAAdjaADAGgzsZiUSEjGuLUxbh2LedsXAKDzIegAANqM40iBgGRZbm1Zbu043vYFAOh8CDoAgDZj21I4LPn9bu33uzWbfwIAOhobhgIA2lQoJMXj7nQ1xyHkAAC8QdABALQ525bKyrzuAgDQmTF1DQAAAEDeIegAAJrFpp8AgFxG0AEANMGmnwCAXEfQAQA0wqafAIB8QNABADTCpp8AgHxA0AEANMKmnwCAfNCqoLN48WINHDhQtm1r+PDhWrNmzUHP//jjjzVt2jT1799fPp9Pxx9/vJ566qlWNQwAaF9s+gkAyAcZ76OzbNkyTZ8+Xffcc4+GDx+uhQsXasyYMdq4caOKi4ubnL9371595StfUXFxsR5++GENGDBAW7ZsUa9evdqifwBAO2DTTwBArrOMqZ+FnZ7hw4frjDPO0KJFiyRJdXV1Kikp0TXXXKOZM2c2Of+ee+7R//7f/1tvvfWWunXr1qomE4mEioqKVF1drUAg0KrXAAAAAJD70s0GGU1d27t3r9auXauKiorPXqCgQBUVFVq9enWz1zzxxBMaMWKEpk2bpmAwqFNOOUW33HKLamtrW/w+NTU1SiQSjR4AAAAAkK6Mgs6uXbtUW1urYDDY6HgwGFRVVVWz10SjUT388MOqra3VU089pZtuukm//OUv9bOf/azF77NgwQIVFRU1PEpKSjJpEwAAAEAn1+6rrtXV1am4uFj//d//raFDh2rSpEm64YYbdM8997R4zaxZs1RdXd3w2LZtW3u3CQB5K5WSolH2wQEAdC4ZLUbQt29fdenSRfF4vNHxeDyufv36NXtN//791a1bN3Xp0qXh2EknnaSqqirt3btXhYWFTa7x+Xzy+XyZtAYAaEYk4m72mUi4S0SHw+5CAwAA5LuMRnQKCws1dOhQRSKRhmN1dXWKRCIaMWJEs9ecddZZevfdd1VXV9dw7O2331b//v2bDTkAgLaRSrkhJ5l062TSrRnZAQB0BhlPXZs+fbqWLFmiBx54QBs2bNDVV1+t3bt3a+rUqZKkyy67TLNmzWo4/+qrr9aHH36oa6+9Vm+//baWL1+uW265RdOmTWu7dwEAaCIWc0dy6tfWNMatYzFv+wIAoCNkvI/OpEmTtHPnTs2ePVtVVVUaMmSIVqxY0bBAwdatW1VQ8Fl+Kikp0dNPP60f/vCH+tKXvqQBAwbo2muv1X/913+13bsAADThOO50tWTSDTmW5W7+6ThedwYAQPvLeB8dL7CPDgC0DvfoAADyTbrZIOMRHQBA7giFpHjcna7mOJJte90RAAAdg6ADAHnOtqWyMq+7AACgY7X7PjoAAAAA0NEIOgAAAADyDkEHAHJEKiVFo+yDAwBAOgg6AJADIhEpGJTKy93nA/ZtBgAAzSDoAECWS6XcJaKTSbdOJt2akR0AAFpG0AGALBeLufvg1O96Zoxbx2Le9gUAQDYj6ABAlnMcd7NPy3Jry3Jrx/G2LwAAshlBBwCynG1L4bDk97u13+/WbP4JAEDL2DAUAHJAKCTF4+50Ncch5AAAcCgEHQDIEbYtlZV53QUAALmBqWsAAAAA8g5BBwAAAEDeIegAQAdKpaRolD1wAABobwQdAOggkYgUDErl5e5zJOJ1RwAA5C+CDgB0gFRKqqyUkkm3TibdmpEdAADaB0EHADpALCYlEpIxbm2MW8di3vYFAEC+IugAQAdwHCkQkCzLrS3LrR3H274AAMhXBB0A6AC2LYXDkt/v1n6/W7PxJwAA7YMNQwGgg4RCUjzuTldzHEIOAADtiaADAB3ItqWyMq+7AAAg/zF1DQAAAEDeIegAAAAAyDsEHQBohVRKikbZBwcAgGxF0AGADEUiUjAolZe7z5GI1x0BAIDPI+gAQAZSKamyUkom3TqZdGtGdgAAyC4EHQDIQCwmJRKSMW5tjFvHYt72BQAAGiPoAEAGHEcKBCTLcmvLcmvH8bYvAADQGEEHADJg21I4LPn9bu33uzWbfwIAkF3YMBQAMhQKSfG4O13NcQg5AABkI4IOALSCbUtlZV53AQAAWsLUNQAAAAB5h6ADAAAAIO8QdAB0WqmUFI2yBw4AAPmIoAOgU4pEpGBQKi93nyMRrzsCAABtiaADoNNJpaTKSimZdOtk0q0Z2QEAIH8QdAB0OrGYlEhIxri1MW4di3nbFwAAaDsEHQCdjuNIgYBkWW5tWW7tON72BQAA2g5BB0CnY9tSOCz5/W7t97s1G38CAJA/2DAUQKcUCknxuDtdzXEIOQAA5BuCDoBOy7alsjKvuwAAAO2BqWsAAAAA8g5BBwAAAEDeIegAyHmplBSNsg8OAAD4DEEHQE6LRKRgUCovd58jEa87AgAA2YCgAyBnpVJSZaWUTLp1MunWjOwAAACCDoCcFYtJiYRkjFsb49axmLd9AQAA7xF0AOQsx5ECAcmy3Nqy3NpxvO0LAAB4j6ADIGfZthQOS36/W/v9bs3mnwAAgA1DAeS0UEiKx93pao5DyAEAAC6CDoCcZ9tSWZnXXQAAgGzC1DUAAAAAeYegAwAAACDvEHQAZI1USopG2QcHAAAcPoIOgKwQiUjBoFRe7j5HIl53BAAAchlBB4DnUimpslJKJt06mXRrRnYAAEBrEXQAeC4WkxIJyRi3NsatYzFv+wIAALmLoAPAc44jBQKSZbm1Zbm143jbFwAAyF0EHQCes20pHJb8frf2+92azT8BAEBrtSroLF68WAMHDpRt2xo+fLjWrFnT4rlLly6VZVmNHja/vQD4nFBIiselTZvc51DI644AAEAuyzjoLFu2TNOnT9ecOXO0bt06DR48WGPGjNGOHTtavCYQCOiDDz5oeGzZsuWwmgaQn2xbKitjJAcAABy+jIPO7bffriuvvFJTp07VySefrHvuuUc9evTQfffd1+I1lmWpX79+DY9gMHhYTQMAAADAwWQUdPbu3au1a9eqoqLisxcoKFBFRYVWr17d4nWffPKJSktLVVJSovHjx+uNN9446PepqalRIpFo9ACQG9j0EwAAZIOMgs6uXbtUW1vbZEQmGAyqqqqq2WtOOOEE3XfffXr88cf1hz/8QXV1dTrzzDP1/vvvt/h9FixYoKKiooZHSUlJJm0C8AibfgIAgGzR7quujRgxQpdddpmGDBmikSNHKhwO66ijjtJvfvObFq+ZNWuWqqurGx7btm1r7zYBHCY2/QQAANmkayYn9+3bV126dFE8Hm90PB6Pq1+/fmm9Rrdu3XTqqafq3XffbfEcn88nn8+XSWsAPFa/6We9Azf9LCvzri8AANA5ZTSiU1hYqKFDhypywHyUuro6RSIRjRgxIq3XqK2t1b/+9S/1798/s04BZDU2/QQAANkk46lr06dP15IlS/TAAw9ow4YNuvrqq7V7925NnTpVknTZZZdp1qxZDefffPPNeuaZZxSNRrVu3Tpdeuml2rJli6644oq2excAPMemnwAAIJtkNHVNkiZNmqSdO3dq9uzZqqqq0pAhQ7RixYqGBQq2bt2qgoLP8tNHH32kK6+8UlVVVTryyCM1dOhQvfTSSzr55JPb7l0AyAr1m37GYu5IDiEHAAB4xTLGGK+bOJREIqGioiJVV1crEAh43Q4AAAAAj6SbDdp91TUAAAAA6GgEHQAAAAB5h6ADoFmplBSNsg8OAADITQQdAE1EIlIwKJWXu88HrCgPAACQEwg6ABpJpaTKSimZdOtk0q0Z2QEAALmEoAOgkVhMSiSk+vUYjXHrWMzbvgAAADJB0AHQiONIgYBkWW5tWW7tON72BQAAkAmCDoBGbFsKhyW/3639frdm808AAJBLunrdAIDsEwpJ8bg7Xc1xCDkAACD3EHQANMu2pbIyr7sAAABoHaauAQAAAMg7BB0AAAAAeYegA+SxVEqKRtkDBwAAdD4EHSBPRSJSMCiVl7vPkYjXHQEAAHQcgg6Qh1IpqbJSSibdOpl0a0Z2AABAZ0HQAfJQLCYlEpIxbm2MW8di3vYFAADQUQg6QB5yHCkQkCzLrS3LrR3H274AAAA6CkEHyEO2LYXDkt/v1n6/W7PxJwAA6CzYMBTIU6GQFI+709Uch5ADAAA6F4IOkMdsWyor87oLAACAjsfUNQAAAAB5h6ADAAAAIO8QdIAckEpJ0Sj74AAAAKSLoANkuUhECgal8nL3ORLxuiMAAIDsR9ABslgqJVVWSsmkWyeTbs3IDgAAwMERdIAsFotJiYRkjFsb49axmLd9AQAAZDuCDpDFHEcKBCTLcmvLcmvH8bYvAACAbEfQAbKYbUvhsOT3u7Xf79Zs/gkAAHBwbBgKZLlQSIrH3elqjkPIAQAASAdBB8gBti2VlXndBQAAQO5g6hoAAACAvEPQAQAAAJB3CDpAB0qlpGiUfXAAAADaG0EH6CCRiBQMSuXl7nMk4nVHAAAA+YugA3SAVEqqrJSSSbdOJt2akR0AAID2QdABOkAsJiUSkjFubYxbx2Le9gUAAJCvCDpAB3AcKRCQLMutLcutHcfbvgAAAPIVQQfoALYthcOS3+/Wfr9bs/knAABA+2DDUKCDhEJSPO5OV3McQg4AAEB7IugAHci2pbIyr7sAAADIf0xdAwAAAJB3CDoAAAAA8g5BB8hQKiVFo+yBAwAAkM0IOkAGIhEpGJTKy93nSMTrjgAAANAcgg6QplRKqqyUkkm3TibdmpEdAACA7EPQAdIUi0mJhGSMWxvj1rGYt30BAACgKYIOkCbHkQIBybLc2rLc2nG87QsAAABNEXSANNm2FA5Lfr9b+/1uzcafAAAA2YcNQ4EMhEJSPO5OV3McQg4AAEC2IugAGbJtqazM6y4AAABwMExdAwAAAJB3CDoAAAAA8g5BB51WKiVFo+yDAwAAkI8IOuiUIhEpGJTKy93nSMTrjgAAANCWCDrodFIpqbJSSibdOpl0a0Z2AAAA8gdBB51OLCYlEpIxbm2MW8di3vYFAACAtkPQQafjOFIgIFmWW1uWWzuOt30BAACg7RB00OnYthQOS36/W/v9bs3mnwAAAPmDDUPRKYVCUjzuTldzHEIOAABAvmnViM7ixYs1cOBA2bat4cOHa82aNWld9+CDD8qyLE2YMKE13xZoU7YtlZURcgAAAPJRxkFn2bJlmj59uubMmaN169Zp8ODBGjNmjHbs2HHQ6zZv3qwZM2bonHPOaXWzAAAAAJCOjIPO7bffriuvvFJTp07VySefrHvuuUc9evTQfffd1+I1tbW1+va3v6158+aprKzssBoGAAAAgEPJKOjs3btXa9euVUVFxWcvUFCgiooKrV69usXrbr75ZhUXF+u73/1uWt+npqZGiUSi0QNoSSolRaPsgwMAAIDPZBR0du3apdraWgWDwUbHg8Ggqqqqmr3mxRdf1L333qslS5ak/X0WLFigoqKihkdJSUkmbaITiUSkYFAqL3efIxGvOwIAAEA2aNflpZPJpCZPnqwlS5aob9++aV83a9YsVVdXNzy2bdvWjl0iV6VSUmWllEy6dTLp1ozsAAAAIKPlpfv27asuXbooHo83Oh6Px9WvX78m52/atEmbN2/WuHHjGo7V1dW537hrV23cuFHl5eVNrvP5fPL5fJm0hk4oFpMOnNVojFvHYu5qagAAAOi8MhrRKSws1NChQxU5YH5QXV2dIpGIRowY0eT8E088Uf/617+0fv36hsfXvvY1jR49WuvXr2dKGg6L40iBgGRZbm1Zbu043vYFAAAA72W8Yej06dM1ZcoUnX766Ro2bJgWLlyo3bt3a+rUqZKkyy67TAMGDNCCBQtk27ZOOeWURtf36tVLkpocBzJl21I47E5XSyQkv9+t2RcHAAAAGQedSZMmaefOnZo9e7aqqqo0ZMgQrVixomGBgq1bt6qgoF1v/QEahEJSPO5OV3McQg4AAABcljHGeN3EoSQSCRUVFam6ulqBQMDrdgAAAAB4JN1swNALAAAAgLxD0AEAAACQdwg6yAqplBSNsgcOAAAA2gZBB56LRKRgUCovd58PWL0cAAAAaBWCDjyVSrnLQyeTbp1MujUjOwAAADgcBB14KhZz98CpX/vPGLeOxbztCwAAALmNoANPOY4UCEiW5daW5daO421fAAAAyG0EHXjKtqVwWPL73drvd2s2/gQAAMDh6Op1A0AoJMXj7nQ1xyHkAAAA4PARdJAVbFsqK/O6CwAAAOQLpq4BAAAAyDsEHbQpNv4EAABANiDooM2w8ScAAACyBUEHbYKNPwEAAJBNCDpoE2z8CQAAgGxC0EGbYONPAAAAZBOCDtoEG38CAAAgm7CPDtoMG38CAAAgWxB00KbY+BMAAADZgKlrAAAAAPIOQQcAAABA3iHooIlUSopG2QMHAAAAuYugg0YiESkYlMrL3edIxOuOAAAAgMwRdNAglZIqK6Vk0q2TSbdmZAcAAAC5hqCDBrGYlEhIxri1MW4di3nbFwAAAJApgg4aOI4UCEiW5daW5daO421fAAAAQKYIOmhg21I4LPn9bu33uzUbfwIAACDXsGEoGgmFpHjcna7mOIQcAAAA5CaCDpqwbamszOsuAAAAgNZj6hoAAACAvEPQAQAAAJB3CDp5LJWSolH2wQEAAEDnQ9DJU5GIFAxK5eXucyTidUcAAABAxyHo5KFUSqqslJJJt04m3ZqRHQAAAHQWBJ08FItJiYRkjFsb49axmLd9AQAAAB2FoJOHHEcKBCTLcmvLcmvH8bYvAAAAoKMQdPKQbUvhsOT3u7Xf79Zs/gkAAIDOgg1D81QoJMXj7nQ1xyHkAAAAoHMh6OQx25bKyrzuAgAAAOh4TF0DAAAAkHcIOgAAAADyDkEnB6RSUjTKPjgAAABAugg6WS4SkYJBqbzcfY5EvO4IAAAAyH4EnSyWSkmVlVIy6dbJpFszsgMAAAAcHEEni8ViUiIhGePWxrh1LOZtXwAAAEC2I+hkMceRAgHJstzastzacbztCwAAAMh2BJ0sZttSOCz5/W7t97s1m38CAAAAB8eGoVkuFJLicXe6muMQcgAAAIB0EHRygG1LZWVedwEAAADkDqauAQAAAMg7BB0AAAAAeYeg00FSKSkaZQ8cAAAAoCMQdDpAJCIFg1J5ufsciXjdEQAAAJDfCDrtLJWSKiulZNKtk0m3ZmQHAAAAaD8EnXYWi0mJhGSMWxvj1rGYt30BAAAA+Yyg084cRwoEJMtya8tya8fxti8AAAAgnxF02pltS+Gw5Pe7td/v1mz8CQAAALQfNgztAKGQFI+709Uch5ADAAAAtDeCTgexbamszOsuAAAAgM6BqWsAAAAA8k6rgs7ixYs1cOBA2bat4cOHa82aNS2eGw6Hdfrpp6tXr1464ogjNGTIEP3+979vdcMAAAAAcCgZB51ly5Zp+vTpmjNnjtatW6fBgwdrzJgx2rFjR7Pn9+7dWzfccINWr16tf/7zn5o6daqmTp2qp59++rCb90IqJUWj7IMDAAAAZDPLmPodXtIzfPhwnXHGGVq0aJEkqa6uTiUlJbrmmms0c+bMtF7jtNNO09ixY/XTn/40rfMTiYSKiopUXV2tQCCQSbttKhJxN/tMJNwlosNhd6EBAAAAAB0j3WyQ0YjO3r17tXbtWlVUVHz2AgUFqqio0OrVqw95vTFGkUhEGzdu1LnnntvieTU1NUokEo0eXkul3JCTTLp1MunWjOwAAAAA2SejoLNr1y7V1tYqGAw2Oh4MBlVVVdXiddXV1erZs6cKCws1duxY3XnnnfrKV77S4vkLFixQUVFRw6OkpCSTNttFLOaO5NSPfxnj1rGYt30BAAAAaKpDVl3z+/1av369XnnlFc2fP1/Tp0/XqlWrWjx/1qxZqq6ubnhs27atI9o8KMdxp6tZlltblls7jrd9AQAAAGgqo310+vbtqy5duigejzc6Ho/H1a9fvxavKygo0LHHHitJGjJkiDZs2KAFCxZo1KhRzZ7v8/nk8/kyaa3d2bZ7T079PTp+v1uz+ScAAACQfTIa0SksLNTQoUMViUQajtXV1SkSiWjEiBFpv05dXZ1qamoy+dZZIRSS4nFp0yb3mYUIAAAAgOyU0YiOJE2fPl1TpkzR6aefrmHDhmnhwoXavXu3pk6dKkm67LLLNGDAAC1YsECSe7/N6aefrvLyctXU1Oipp57S73//e919991t+046iG1LZWVedwEAAADgYDIOOpMmTdLOnTs1e/ZsVVVVaciQIVqxYkXDAgVbt25VQcFnA0W7d+/W97//fb3//vvq3r27TjzxRP3hD3/QpEmT2u5dAAAAAMABMt5HxwvZso8OAAAAAG+1yz46AAAAAJALCDoAAAAA8g5BBwAAAEDeIegAAAAAyDsEHQAAAAB5h6ADAAAAIO8QdAAAAADkHYIOAAAAgLxD0AEAAACQdwg6AAAAAPIOQQcAAABA3iHoAAAAAMg7BB0AAAAAeYegAwAAACDvEHQAAAAA5B2CDgAAAIC809XrBtJhjJEkJRIJjzsBAAAA4KX6TFCfEVqSE0EnmUxKkkpKSjzuBAAAAEA2SCaTKioqavHrljlUFMoCdXV1isVi8vv9sizL014SiYRKSkq0bds2BQIBT3tB7uHzg8PB5wetxWcHh4PPDw5He3x+jDFKJpNyHEcFBS3fiZMTIzoFBQU6+uijvW6jkUAgwA87Wo3PDw4Hnx+0Fp8dHA4+Pzgcbf35OdhITj0WIwAAAACQdwg6AAAAAPIOQSdDPp9Pc+bMkc/n87oV5CA+PzgcfH7QWnx2cDj4/OBwePn5yYnFCAAAAAAgE4zoAAAAAMg7BB0AAAAAeYegAwAAACDvEHQAAAAA5B2CDgAAAIC8Q9BpxuLFizVw4EDZtq3hw4drzZo1Bz3/z3/+s0488UTZtq1Bgwbpqaee6qBOkY0y+fwsWbJE55xzjo488kgdeeSRqqioOOTnDfkr07976j344IOyLEsTJkxo3waR1TL9/Hz88ceaNm2a+vfvL5/Pp+OPP57//+rEMv38LFy4UCeccIK6d++ukpIS/fCHP1QqleqgbpEtnn/+eY0bN06O48iyLD322GOHvGbVqlU67bTT5PP5dOyxx2rp0qXt1h9B53OWLVum6dOna86cOVq3bp0GDx6sMWPGaMeOHc2e/9JLL+niiy/Wd7/7Xb366quaMGGCJkyYoNdff72DO0c2yPTzs2rVKl188cVauXKlVq9erZKSEp1//vnavn17B3cOr2X62am3efNmzZgxQ+ecc04HdYpslOnnZ+/evfrKV76izZs36+GHH9bGjRu1ZMkSDRgwoIM7RzbI9PPzxz/+UTNnztScOXO0YcMG3XvvvVq2bJl+8pOfdHDn8Nru3bs1ePBgLV68OK3z33vvPY0dO1ajR4/W+vXrdd111+mKK67Q008/3T4NGjQybNgwM23atIa6trbWOI5jFixY0Oz5EydONGPHjm10bPjw4eZ//a//1a59Ijtl+vn5vP379xu/328eeOCB9moRWao1n539+/ebM8880/z2t781U6ZMMePHj++ATpGNMv383H333aasrMzs3bu3o1pEFsv08zNt2jRz3nnnNTo2ffp0c9ZZZ7Vrn8huksyjjz560HOuv/5688UvfrHRsUmTJpkxY8a0S0+M6Bxg7969Wrt2rSoqKhqOFRQUqKKiQqtXr272mtWrVzc6X5LGjBnT4vnIX635/Hzep59+qn379ql3797t1SayUGs/OzfffLOKi4v13e9+tyPaRJZqzefniSee0IgRIzRt2jQFg0GdcsopuuWWW1RbW9tRbSNLtObzc+aZZ2rt2rUN09ui0aieeuopXXjhhR3SM3JXR//e3LVdXjVH7dq1S7W1tQoGg42OB4NBvfXWW81eU1VV1ez5VVVV7dYnslNrPj+f91//9V9yHKfJXwLIb6357Lz44ou69957tX79+g7oENmsNZ+faDSqv/3tb/r2t7+tp556Su+++66+//3va9++fZozZ05HtI0s0ZrPzyWXXKJdu3bp7LPPljFG+/fv11VXXcXUNRxSS783JxIJ7dmzR927d2/T78eIDpAlbr31Vj344IN69NFHZdu21+0giyWTSU2ePFlLlixR3759vW4HOaiurk7FxcX67//+bw0dOlSTJk3SDTfcoHvuucfr1pADVq1apVtuuUV33XWX1q1bp3A4rOXLl+unP/2p160BjTCic4C+ffuqS5cuisfjjY7H43H169ev2Wv69euX0fnIX635/NS77bbbdOutt+rZZ5/Vl770pfZsE1ko08/Opk2btHnzZo0bN67hWF1dnSSpa9eu2rhxo8rLy9u3aWSN1vzd079/f3Xr1k1dunRpOHbSSSepqqpKe/fuVWFhYbv2jOzRms/PTTfdpMmTJ+uKK66QJA0aNEi7d+/W9773Pd1www0qKODf0dG8ln5vDgQCbT6aIzGi00hhYaGGDh2qSCTScKyurk6RSEQjRoxo9poRI0Y0Ol+S/vrXv7Z4PvJXaz4/kvSLX/xCP/3pT7VixQqdfvrpHdEqskymn50TTzxR//rXv7R+/fqGx9e+9rWGVWxKSko6sn14rDV/95x11ll69913GwKyJL399tvq378/IaeTac3n59NPP20SZupDs3tPOtC8Dv+9uV2WOMhhDz74oPH5fGbp0qXmzTffNN/73vdMr169TFVVlTHGmMmTJ5uZM2c2nP/3v//ddO3a1dx2221mw4YNZs6cOaZbt27mX//6l1dvAR7K9PNz6623msLCQvPwww+bDz74oOGRTCa9egvwSKafnc9j1bXOLdPPz9atW43f7zc/+MEPzMaNG82TTz5piouLzc9+9jOv3gI8lOnnZ86cOcbv95s//elPJhqNmmeeecaUl5ebiRMnevUW4JFkMmleffVV8+qrrxpJ5vbbbzevvvqq2bJlizHGmJkzZ5rJkyc3nB+NRk2PHj3Mj3/8Y7NhwwazePFi06VLF7NixYp26Y+g04w777zTfOELXzCFhYVm2LBh5uWXX2742siRI82UKVManf/QQw+Z448/3hQWFpovfvGLZvny5R3cMbJJJp+f0tJSI6nJY86cOR3fODyX6d89ByLoINPPz0svvWSGDx9ufD6fKSsrM/Pnzzf79+/v4K6RLTL5/Ozbt8/MnTvXlJeXG9u2TUlJifn+979vPvroo45vHJ5auXJls7/H1H9epkyZYkaOHNnkmiFDhpjCwkJTVlZm7r///nbrzzKGMUYAAAAA+YV7dAAAAADkHYIOAAAAgLxD0AEAAACQdwg6AAAAAPIOQQcAAABA3iHoAAAAAMg7BB0AAAAAeYegAwAAACDvEHQAAAAA5B2CDgAAAIC8Q9ABAAAAkHf+H2kR6vM1pqIhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds_new = model_0(X_test)\n",
    "    \n",
    "plot_predictions(predictions=y_preds)\n",
    "plot_predictions(predictions=y_preds_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
